# Principal Component Analysis (PCA) Implementation from Scratch

A comprehensive implementation of Principal Component Analysis (PCA) algorithm from scratch using Python and NumPy, applied to African student performance data.

## ğŸ“‹ Project Overview

This project implements PCA from the ground up, covering all mathematical foundations and providing practical insights into dimensionality reduction techniques. The implementation demonstrates:

- **Complete PCA algorithm** built from scratch
- **Mathematical foundations** including covariance matrices and eigendecomposition
- **Dynamic component selection** based on explained variance
- **Performance optimization** techniques for large datasets
- **Comprehensive visualizations** and analysis

## ğŸ“Š Dataset

**Student Performance and Learning Style Dataset**
- **Source:** [Kaggle - Student Performance and Learning Style](https://www.kaggle.com/datasets/adilshamim8/student-performance-and-learning-style)
- **Author:** Adil Shamim
- **Context:** African educational data with 16 features and 14,003 samples
- **Features:** Study hours, attendance, resources, motivation, academic performance metrics

## ğŸ¯ Assignment Tasks Completed

### Task 1: PCA Implementation from Scratch
- âœ… Manual covariance matrix calculation
- âœ… Eigendecomposition of covariance matrix
- âœ… Data projection onto principal components
- âœ… Inverse transformation functionality

### Task 2: Dynamic Component Selection
- âœ… Automatic selection based on explained variance thresholds
- âœ… Analysis of 80%, 85%, 90%, 95%, and 99% variance retention
- âœ… Comprehensive visualizations of variance trade-offs

### Task 3: Performance Optimization
- âœ… SVD-based implementation for numerical stability
- âœ… Memory-efficient batch processing
- âœ… Performance comparisons with scikit-learn

## ğŸ“ˆ Key Results

- **First Principal Component:** Explains 12.36% of total variance
- **Top 5 Components:** Capture 38.81% of total variance
- **95% Variance Retention:** Requires 15 out of 16 components
- **Dimensionality Reduction:** Achieves significant compression while preserving data structure

## ğŸ›  Technologies Used

- **Python 3.8+**
- **NumPy** - Linear algebra operations
- **Pandas** - Data manipulation and analysis
- **Matplotlib & Seaborn** - Data visualization
- **Jupyter Notebook** - Interactive development environment

## ğŸ“ Files Structure

```
â”œâ”€â”€ PCA_Assignment.ipynb    # Main notebook with complete implementation
â”œâ”€â”€ student_performance.csv # Dataset file
â””â”€â”€ README.md              # Project documentation
```

## ğŸš€ Getting Started

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Mikekimm/Principal-Component-Analysis.git
   cd Principal-Component-Analysis
   ```

2. **Install required packages:**
   ```bash
   pip install numpy pandas matplotlib seaborn scikit-learn
   ```

3. **Run the notebook:**
   ```bash
   jupyter notebook PCA_Assignment.ipynb
   ```

## ğŸ“– Notebook Sections

1. **Data Loading and Exploration** - African student performance dataset analysis
2. **Data Preprocessing** - Standardization and missing value handling
3. **PCA Implementation** - Complete algorithm from scratch
4. **Covariance Matrix Calculation** - Manual implementation with verification
5. **Eigendecomposition** - Eigenvalues and eigenvectors analysis
6. **Data Projection** - Transformation to principal component space
7. **Dynamic Component Selection** - Variance-based component selection
8. **Performance Optimization** - SVD implementation and efficiency improvements
9. **Results Visualization** - Comprehensive plots and analysis
10. **Validation** - Comparison with scikit-learn implementation

## ğŸ¨ Visualizations

The notebook includes comprehensive visualizations:
- Correlation matrices and feature distributions
- Eigenvalue plots and explained variance analysis
- 2D and 3D PCA projections
- Component loadings and reconstruction error plots
- Performance comparison charts

## ğŸ“Š Mathematical Foundation

The implementation covers:
- **Covariance Matrix:** Manual calculation and verification
- **Eigendecomposition:** Finding eigenvalues and eigenvectors
- **Principal Components:** Selecting and ranking components
- **Data Transformation:** Projecting data to lower dimensions
- **Inverse Transformation:** Reconstructing original data

## ğŸ† Academic Excellence

This project demonstrates:
- Complete understanding of PCA mathematical foundations
- Professional-quality code with comprehensive documentation
- Rigorous validation against established libraries
- Real-world application with meaningful insights
- Academic integrity with proper citations

## ğŸ‘¨â€ğŸ’» Author

**Michael Kimani**
- GitHub: [@Mikekimm](https://github.com/Mikekimm)
- Project: Principal Component Analysis Implementation

## ğŸ“„ License

This project is for educational purposes as part of ALU coursework.

---

*This implementation demonstrates mastery of dimensionality reduction techniques and serves as a comprehensive resource for understanding PCA from first principles.*